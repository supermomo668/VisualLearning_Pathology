{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpO5NE34z3Ty"
   },
   "source": [
    "Data from: http://zhao-nas.bio.cmu.edu:5000/fsdownload/aBDx29J7H/Ensemble%20learning%20data_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_GBOIxuOFq8",
    "outputId": "54418c39-8744-4654-8965-c7a0b986dd52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3m-m\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install pytorch_lightning\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "783KVUhBVn9-"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2 , os, numpy as np, torch, pandas as pd, tqdm as tqdm, PIL.Image as Image, time, IPython\n",
    "#from pylab import rcParams\n",
    "import datetime\n",
    "# \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "#import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loops.fit_loop import FitLoop\n",
    "from pytorch_lightning.loops.loop import Loop\n",
    "from albumentations.pytorch.transforms import ToTensorV2 \n",
    "#\n",
    "from numpy.lib.function_base import select\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import pytorch_lightning as pl, torchmetrics\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "F3Rn6pJjXGPh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'proj_path': WindowsPath('C:/data/MattM_Ensemble'), 'model_path': WindowsPath('C:/data/MattM_Ensemble/model_chkpts'), 'data_path': WindowsPath('C:/data'), 'dataindex_fn': 'dataIndex.csv', 'dataindex_path': WindowsPath('C:/data/dataIndex.csv'), 'data_name': ['HE_RBG_Corp_images'], 'class_names': ['Responder', 'NonResponder'], '__dict__': <attribute '__dict__' of 'PATH_ARGS' objects>, '__weakref__': <attribute '__weakref__' of 'PATH_ARGS' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "class PATH_ARGS:\n",
    "    proj_path = Path('./').absolute()  # [CHANGE THIS for new environment]\n",
    "    model_path = proj_path/'model_chkpts'\n",
    "    # data path\n",
    "    #data_path = proj_path/'TestingData'   # Test path\n",
    "    #data_path = proj_path/'Ensemble_learning data'      # [CONFIRM THIS for new environment]\n",
    "    data_path = proj_path.parent\n",
    "    dataindex_fn = 'dataIndex.csv'\n",
    "    dataindex_path = data_path/dataindex_fn\n",
    "    # 2 types of images (HE  FISH)\n",
    "    data_name = ['HE_RBG_Corp_images']\n",
    "    #data_name = ['HE images', 'HIPT_AGH_FluorescentImage_R1']\n",
    "    # 2 groups to classify\n",
    "    class_names = ['Responder','NonResponder']\n",
    "    \n",
    "print(PATH_ARGS.__dict__)\n",
    "def mkdirifNE(p):\n",
    "    if not os.path.exists(p): os.mkdir(p)\n",
    "\n",
    "mkdirifNE(PATH_ARGS.model_path)\n",
    "\n",
    "def load_img(img_paths: list, is_mask=False):\n",
    "        \"\"\" load array from a list of image paths \"\"\"\n",
    "        if is_mask: flag = 0\n",
    "        else: flag = -1\n",
    "        return np.concatenate([np.expand_dims(cv2.imread(str(img_fp), flag), axis=0)\n",
    "                               for img_fp in img_paths.tolist()])\n",
    "def normalize(ratios):\n",
    "    \"\"\"normalize a list of ratios to sum to 1\"\"\"\n",
    "    return [r/sum(ratios) for r in ratios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IC1aUI5cbwld"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent_path</th>\n",
       "      <th>type</th>\n",
       "      <th>tissue</th>\n",
       "      <th>x_img_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">C:\\data\\HE_RBG_Corp_images_processed\\NonResponder</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">HE_RBG_Corp_images</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638).tif</th>\n",
       "      <th>NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638)_t0.tif</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638)_t1.tif</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   label  \\\n",
       "parent_path                                       type               tissue                                             x_img_path                                                         \n",
       "C:\\data\\HE_RBG_Corp_images_processed\\NonResponder HE_RBG_Corp_images NA-24_0000023786_2021-03-22 09_49_45.scn - Seri... NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  NonResponder   \n",
       "                                                                                                                        NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  NonResponder   \n",
       "\n",
       "                                                                                                                                                                              set  \n",
       "parent_path                                       type               tissue                                             x_img_path                                                 \n",
       "C:\\data\\HE_RBG_Corp_images_processed\\NonResponder HE_RBG_Corp_images NA-24_0000023786_2021-03-22 09_49_45.scn - Seri... NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  train  \n",
       "                                                                                                                        NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  train  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index_df = pd.read_csv(PATH_ARGS.data_path/'dataIndex.csv', index_col=list(range(4)))\n",
    "# print(data_index_df['set'].unique())\n",
    "# print(data_index_df['set'].value_counts())\n",
    "data_index_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjnyh2KKfp3L"
   },
   "source": [
    "### Dataloader - loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oVqkEUQlftyf"
   },
   "outputs": [],
   "source": [
    "class META_ARGS:\n",
    "    RANDOM_SEED = 42\n",
    "    INPUT_DIM = (224,224)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DATA_ARGS:\n",
    "    num_classes = 2\n",
    "    batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Rvj6sDvItzfJ"
   },
   "outputs": [],
   "source": [
    "def _get_normalize_attributes(data_index_df):\n",
    "    x_imgs = load_img(data_index_df['x_img_path'])\n",
    "    means, stds = np.mean(x_imgs, axis=((0,1,2))), np.std(x_imgs, axis=((0,1,2)))\n",
    "    return means, stds\n",
    "\n",
    "# dataloader\n",
    "class HEData(Dataset):\n",
    "    def __init__(self, dataindex_df: pd.DataFrame,\n",
    "                 x_img_cols:str=['x_img_path'], y_cols:list=['label'],\n",
    "                 transform=None, target_transform=None,\n",
    "                 debug:bool=False):\n",
    "        \"\"\" \n",
    "        parameters\n",
    "            csv_file: contain indexer file\n",
    "            \n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        # \n",
    "        self.n = len(dataindex_df)\n",
    "        # fetch individual \n",
    "        self.y_ds = dataindex_df[y_cols]\n",
    "        self.num_classes = self.y_ds.nunique()\n",
    "        self.y_ds_enc = self.label_encode(self.y_ds, oh=False)\n",
    "        # \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.debug:\n",
    "            print(f\"Target shape:{self.y_ds_enc.shape}\")\n",
    "            print(f\"[INFO]Image classes: {self.num_classes} with {self.n} instances.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def label_encode(self, ys, oh:bool=False):\n",
    "        # encode target label\n",
    "        if oh:\n",
    "            self.enc = OneHotEncoder()\n",
    "            return self.enc.fit_transform(ys).toarray()\n",
    "        else:\n",
    "            self.enc = LabelBinarizer()\n",
    "        ys_enc = self.enc.fit_transform(ys)\n",
    "        return ys_enc.flatten()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input images\n",
    "        if self.debug: print(f\"Instance series: {self.y_ds.iloc[idx]},{self.y_ds.iloc[idx].name}, {idx}\")\n",
    "        parent_path, _, _, tile_name = self.y_ds.iloc[idx].name   # parent, type, source_tissue, tile_name\n",
    "            # get data\n",
    "        x_data = np.array(Image.open(Path(parent_path)/tile_name))\n",
    "        y_data = self.y_ds_enc[idx]    #.reshape((-1,))\n",
    "        if self.transform is not None:\n",
    "            x_data = self.transform(image=x_data)['image']\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y_data = self.target_transform(y_data)\n",
    "        # outputs g(t)\n",
    "        if self.debug: print(x_data.shape, x_data.dtype, y_data.shape, y_data.dtype)\n",
    "        return torch.tensor(x_data).float(), torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "def get_transforms(target_size=(224,224), get_normalizing_attributes:bool=False, data_index_df:pd.DataFrame=False):    \n",
    "    assert bool(get_normalizing_attributes) == bool(data_index_df), \"must be provided together\"\n",
    "    p1 = 0.1\n",
    "    p2 = 0.05\n",
    "    p3 = 0.2\n",
    "\n",
    "    if get_normalizing_attributes:\n",
    "        im_means, im_stds = _get_normalize_attributes()\n",
    "    else:   # use pre-computed values\n",
    "        im_means, im_stds=[0, 0, 0], [1, 1, 1]\n",
    "    ## Transforms\n",
    "    process_transform = A.Compose([\n",
    "        ToTensorV2(),\n",
    "    ]) # Normalize by channel means, stds\n",
    "    color_transform = A.Compose([\n",
    "        # In-place transformations\n",
    "        A.RandomBrightnessContrast(p=p2),\n",
    "        A.RandomGamma(gamma_limit=(80, 200), p=p3),\n",
    "        A.Blur(blur_limit=7, p=p2),\n",
    "        A.ToGray(p=p2),\n",
    "        A.CLAHE(p=p2),\n",
    "        A.ChannelDropout(channel_drop_range=(1, 2), fill_value=0, p=p2),\n",
    "        A.ChannelShuffle(p=p2),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.2,\n",
    "            always_apply=False,\n",
    "            p=p2,\n",
    "        ),\n",
    "        A.Equalize(mode=\"cv\", by_channels=True, mask=None, mask_params=(), p=p2),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, p=p2),\n",
    "        A.Posterize(num_bits=4, p=p2),\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=p2),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=p1)\n",
    "        #A.GaussianBlur(11, sigma=(0.1, 2.0)),\n",
    "    ])\n",
    "    geometric_transform = A.Compose([\n",
    "        A.Affine(\n",
    "            scale=(0.60, 1.60),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            translate_percent=(-0.2, 0.2),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            rotate=(-30, 30),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            shear=(-20, 20),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        # A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=pt),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ])\n",
    "    ###\n",
    "    transformers = {'process': process_transform,  \n",
    "                    'color': color_transform, 'geometric': geometric_transform}\n",
    "    set_transformers = {'train': A.Compose(color_transform.transforms + geometric_transform.transforms+process_transform.transforms),\n",
    "                        'val': A.Compose(process_transform.transforms),\n",
    "                        'test': A.Compose(process_transform.transforms)}\n",
    "    return set_transformers\n",
    "    #return transformers\n",
    "    \n",
    "#HEData(dataindex_df=pd.read_csv(PATH_ARGS.dataindex_path,index_col=list(range(4))))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "_iadyH-ar2kB"
   },
   "outputs": [],
   "source": [
    "# full dataset objecct\n",
    "class HEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, dataindex_path=Path('./dataIndex.csv'), label_col='label', debug=False):\n",
    "        super().__init__()\n",
    "        self.dataindex_path = Path(dataindex_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = DATA_ARGS.num_classes\n",
    "        self.label_col = label_col\n",
    "        self.transforms = get_transforms()\n",
    "        self.debug = debug\n",
    "        print(f\"Debug mode:{self.debug}\")\n",
    "        self.index_col_len = 4\n",
    "    \n",
    "    def get_sampler(self, dataset):\n",
    "        \"\"\"get sampler if needed\"\"\"\n",
    "        if self.label_col:\n",
    "            class_cts = dataset[self.label_col].value_counts()\n",
    "            for label in class_cts.index:\n",
    "                class_cts.loc[label] = len(dataset)/class_cts.loc[label]\n",
    "            weights = np.zeros(len(dataset))\n",
    "            for label in class_cts.index:\n",
    "                weights[np.where(dataset[self.label_col].to_numpy()==label)[0]] = class_cts.loc[label]\n",
    "            class_balance_sampler = WeightedRandomSampler(weights, len(dataset), replacement=True)\n",
    "        else:\n",
    "            class_balance_sampler = None\n",
    "        return class_balance_sampler\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.datasets = dict()\n",
    "        self.sampler = dict()\n",
    "        # ['train', 'test', 'val']\n",
    "        dataindex_df = pd.read_csv(self.dataindex_path, index_col=list(range(self.index_col_len)))\n",
    "        dataindex_df = dataindex_df[dataindex_df['set'].isnull()!=True]\n",
    "        for dset in dataindex_df['set'].unique():\n",
    "            self.sampler[dset] = self.get_sampler(dataindex_df[dataindex_df['set']==dset])\n",
    "            self.datasets[dset] = HEData(dataindex_df[dataindex_df['set']==dset],\n",
    "                                         transform = self.transforms[dset], debug=self.debug)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "          self.datasets['train'], batch_size=self.batch_size, shuffle=False if self.sampler['train'] else True, sampler=self.sampler['train'])\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(\n",
    "          self.datasets['val'], batch_size=self.batch_size, shuffle=False if self.sampler['val'] else True, sampler=self.sampler['val'])       \n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-2zwr80fr_0"
   },
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "BZjWd492DsUb"
   },
   "outputs": [],
   "source": [
    "# model and train args\n",
    "class MODEL_ARGS:\n",
    "    n_classes = len(PATH_ARGS.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet_B7_Weights, ResNeXt101_32X8D_Weights, MobileNet_V3_Large_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "gcE8La0ftMU4"
   },
   "outputs": [],
   "source": [
    "# returns the size of the output tensor going into Linear layer from the conv block.\n",
    "def _get_conv_output(self, shape):\n",
    "    batch_size = 1\n",
    "    input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "    output_feat = self._forward_features(input) \n",
    "    n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "    return n_size\n",
    "    \n",
    "#from torch._C import device\n",
    "class HEClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, model_name:str, n_classes:int=2, pretrain:bool=True,\n",
    "                 custom_classification_head:bool=False, input_size:tuple=(224,224), debug:bool=False):\n",
    "        super().__init__()\n",
    "        print(f\"Using pre-trained head:{model_name}\")\n",
    "        avail_models =  ['mobilenetv3','resnext101','efficientnetb7','resnet50']\n",
    "        assert model_name in ['mobilenetv3','resnext101','efficientnetb7','resnet50'], f\"Must be one of {avail_models}\"\n",
    "        self.debug = debug\n",
    "        self.n_classes = n_classes\n",
    "        self.custom_classification_head = custom_classification_head\n",
    "        # Step 1: Initialize model with the weights\n",
    "        if model_name == 'mobilenetv3':\n",
    "            self.model = models.mobilenet_v3_large(pretrained=MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "        elif model_name == 'resnext101':\n",
    "            self.model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n",
    "        elif model_name == 'efficientnetb7':\n",
    "            self.model = models.efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "        elif model_name =='resnet50':\n",
    "            self.model = models.resnet50(pretrained=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        # replace/remove head\n",
    "        removed = list(self.model.children())[:-1]\n",
    "        self.model_base = torch.nn.Sequential(*removed)  \n",
    "        in_feats = self._get_output_feat(self.model_base, input_size)\n",
    "            # head\n",
    "        if self.custom_classification_head:\n",
    "            self.model_head = self.classification_head()\n",
    "        else:\n",
    "            self.model_head = nn.Sequential(nn.Flatten(),\n",
    "                                            nn.Linear(in_features=in_feats, out_features=self.n_classes, bias=True),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.LogSoftmax(dim=1) if n_classes>2 else nn.Sigmoid(),\n",
    "                                           )\n",
    "        self.model = torch.nn.Sequential(self.model_base, self.model_head)\n",
    "            #self.model_head.to(device=META_ARGS.device)     \n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        #self.ROC = torchmetrics.ROC(num_classes=n_classes)\n",
    "        self.AUROC = torchmetrics.AUROC(num_classes=n_classes, pos_label=1)\n",
    "    \n",
    "    def _get_output_feat(self, model, in_shape=(224,224)):\n",
    "        x = torch.randn((3,)+in_shape)\n",
    "        return model(x.unsqueeze(0)).flatten().size()[0]\n",
    "\n",
    "    def _forward_feature_extract(self, x):\n",
    "        return self.model_base(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "#         #x = self.model_head(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(nn.Linear(in_features=self.model.classifier[-1].in_features, out_features=self.n_classes, bias=True)(x))\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        #self.model.classifier = nn.Sequential(*self.model.classifier, nn.Softmax())\n",
    "        if self.debug: print(f\"Num classes:{self.n_classes}\\nModel classifier\\n:{self.model_head}\")\n",
    "        return x\n",
    "\n",
    "    def add_classification_head(self):\n",
    "        #n_features = self.model_head.fc.in_features\n",
    "        classifier_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.model_base.classifier[1].in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512 , 256),\n",
    "            nn.Linear(256 , self.n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "            )\n",
    "        return classifier_layer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-10)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def get_loss(self, y_hat, y):\n",
    "        #loss = nn.CrossEntropyLoss()   # does softmax for you (no need in classifcation)\n",
    "        #loss = nn.LogSoftmax()\n",
    "        #loss = F.nll_loss\n",
    "        if self.debug: print(y.size(), y.dtype, y_hat.size(), y_hat.dtype)\n",
    "        return F.cross_entropy(y_hat,  y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.get_loss(y_hat, y)\n",
    "        # training metrics\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        # optimize (done under the hoood)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "        #return self.get_loss(y, y_hat)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # compute metrics\n",
    "        val_loss =self.get_loss(y_hat, y)\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        auroc = self.AUROC(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        #fpr, tpr, thresholds = self.ROC(y_hat, y)\n",
    "        #\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        self.AUROC.update(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        self.log(\"validation_auc\", self.AUROC, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_auc\", valid_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "\n",
    "class HEEnsembleModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 ensembles_settings:dict={'efficientnetb7':3, 'resnext101':2}, \n",
    "                 n_classes:int=2,\n",
    "                 input_shape=(224,224),\n",
    "                 debug=False):\n",
    "        super(HEEnsembleModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        models = []\n",
    "        self.n_models = 0\n",
    "        for name, number in ensembles_settings.items():\n",
    "            [models.append(\n",
    "                HEClassificationModel(model_name=name, \n",
    "                                      n_classes=2, \n",
    "                                      pretrain=True,\n",
    "                                      custom_classification_head=False\n",
    "                                     )\n",
    "                         ) for i in range(number)\n",
    "            ]\n",
    "            self.n_models += number\n",
    "        self.ensemble_model = torch.nn.ModuleList(models)\n",
    "        self.classifier = torch.nn.Linear(self.n_models*n_classes, n_classes)\n",
    "        #self.save_hyperparameters() # Uncomment to show error\n",
    "        self.CEloss = nn.CrossEntropyLoss()\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        #self.ROC = torchmetrics.ROC(num_classes=n_classes)\n",
    "        self.AUROC = torchmetrics.AUROC(num_classes=n_classes, pos_label=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output=[]\n",
    "        for m in self.ensemble_model:\n",
    "            output.append(m(x))\n",
    "        combined = torch.concat(output,dim=1)\n",
    "        x = self.classifier(combined)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-10)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def get_loss(self, y_hat, y):\n",
    "        #loss = nn.CrossEntropyLoss()   # does softmax for you (no need in classifcation)\n",
    "        #loss = nn.LogSoftmax()\n",
    "        #loss = F.nll_loss\n",
    "        if self.debug: print(y.size(), y.dtype, y_hat.size(), y_hat.dtype)\n",
    "        return self.CEloss(y_hat,  y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.get_loss(y_hat, y)\n",
    "        # training metrics\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        # optimize (done under the hoood)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "        #return self.get_loss(y, y_hat)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # compute metrics\n",
    "        val_loss =self.get_loss(y_hat, y)\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        auroc = self.AUROC(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        #fpr, tpr, thresholds = self.ROC(y_hat, y)\n",
    "        #\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        self.AUROC.update(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        self.log(\"validation_auc\", self.AUROC, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_auc\", valid_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "ZUgCx55pqolP"
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "class PRMetrics(pl.Callback):\n",
    "    \"\"\" Custom callback to compute per-class PR & ROC curves\n",
    "    at the end of each training epoch\n",
    "    \"\"\"\n",
    "    def __init__(self,  val_samples, num_samples=32, class_names={'Non-responder':0, 'Responder':1}):    #generator=None, num_log_batches=1):\n",
    "        # self.generator = generator\n",
    "        # self.num_batches = num_log_batches\n",
    "        # # store full names of classes\n",
    "        # self.class_names = { v: k for k, v in generator.class_indices.items() }\n",
    "        # self.flat_class_names = [k for k, v in generator.class_indices.items()]\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.class_names = class_names\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module, logs={}):\n",
    "        # # collect validation data and ground truth labels from generator\n",
    "        # val_data, val_labels = zip(*(self.generator[i] for i in range(self.num_batches)))\n",
    "        # val_data, val_labels = np.vstack(val_data), np.vstack(val_labels)\n",
    "\n",
    "        # # use the trained model to generate predictions for the given number\n",
    "        # # of validation data batches (num_batches)\n",
    "        # val_predictions = self.model.predict(val_data)\n",
    "        # ground_truth_class_ids = val_labels.argmax(axis=1)\n",
    "        # Bring the tensors to CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        preds = torch.argmax(pl_module(val_imgs), -1)\n",
    "        # Log precision-recall curve the key \"pr_curve\" is the id of the plot--do not change this if you want subsequent runs to show up on the same plot\n",
    "        wandb.log({\"roc_curve\" : wandb.plot.roc_curve(val_labels, preds, labels=self.class_names)})\n",
    "\n",
    "class ImagePredictionLogger(pl.Callback):\n",
    "    \"\"\" callback\"\"\"\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Bring the tensors to CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "        # Log the images as wandb Image\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
    "                                                 preds[:self.num_samples], \n",
    "                                                 val_labels[:self.num_samples])]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl1KS1Qdq_05"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Kinjz3AgaGjB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GradientAccumulationScheduler\n",
    "import pytorch_lightning as pl\n",
    "# logger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "#\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "WvChBn4WJzWb"
   },
   "outputs": [],
   "source": [
    "class TRAIN_args:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 1\n",
    "    epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Gk2iGQLALW7W"
   },
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "        Try resetting model weights to avoid\n",
    "        weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "1aoYeC2oNT2A",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=2)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained head:efficientnetb7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained head:efficientnetb7\n",
      "Using pre-trained head:efficientnetb7\n",
      "Using pre-trained head:resnext101\n",
      "Using pre-trained head:resnext101\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT (ie: no accumulated grads)\n",
    "cbs = [\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss', dirpath=PATH_ARGS.model_path,\n",
    "                                 filename='models-{epoch:02d}-{val_loss:.2f}', save_top_k=2, mode='min'),\n",
    "    EarlyStopping(monitor=\"val_loss\", min_delta=1e-7, patience=8, mode=\"min\"),\n",
    "    GradientAccumulationScheduler(scheduling={0: 8, 4: 4, 8: 1}),\n",
    "    #PRMetrics(),\n",
    "]\n",
    "trainer = pl.Trainer(\n",
    "    #gpus=1, \n",
    "    devices=1,\n",
    "    logger=WandbLogger(project='AD-ensemble(draft)',  entity=\"3m-m\", job_type='train'),\n",
    "    max_epochs=TRAIN_args.epochs, callbacks=cbs)\n",
    "\n",
    "model = HEEnsembleModel(ensembles_settings={'efficientnetb7':3, 'resnext101':2}, \n",
    "                        input_shape=(224,224),\n",
    "                        n_classes=MODEL_ARGS.n_classes,\n",
    "                       debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBQBmVGE3u_M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | ensemble_model | ModuleList       | 364 M \n",
      "1 | classifier     | Linear           | 22    \n",
      "2 | CEloss         | CrossEntropyLoss | 0     \n",
      "3 | accuracy       | Accuracy         | 0     \n",
      "4 | AUROC          | AUROC            | 0     \n",
      "----------------------------------------------------\n",
      "364 M     Trainable params\n",
      "0         Non-trainable params\n",
      "364 M     Total params\n",
      "1,459.477 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_14688\\3542098707.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x_data).float(), torch.tensor(y_data, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Argument `pos_label` should be `None` when running multiclass precision recall curve. Got 1\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e330553511d94e78b383673b0e8f4e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n",
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n",
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n",
      "torch.Size([8]) torch.int64 torch.Size([8, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "datamodule = HEDataModule(batch_size=DATA_ARGS.batch_size, dataindex_path=PATH_ARGS.dataindex_path, debug=False)\n",
    "datamodule.setup()\n",
    "trainer.fit(model=model, datamodule=datamodule) \n",
    "# save with parameters\n",
    "#torch.save([model.kwargs, model.state_dict()], path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v65ctfW7HABS"
   },
   "source": [
    "### Prediction/submission"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "uUsLsFHLv8b8"
   },
   "source": [
    "test_loader = DataLoader(HEdatasets['test'], shuffle=True, batch_size=TRAIN_args.test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dca68bea1ee5b3be888f6b0e1f9475265928937e00d73db3ed64c34751d2276f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
