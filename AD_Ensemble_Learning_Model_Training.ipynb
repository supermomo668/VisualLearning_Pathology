{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpO5NE34z3Ty"
   },
   "source": [
    "Data from: http://zhao-nas.bio.cmu.edu:5000/fsdownload/aBDx29J7H/Ensemble%20learning%20data_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_GBOIxuOFq8",
    "outputId": "54418c39-8744-4654-8965-c7a0b986dd52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3m-m\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install pytorch_lightning\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "783KVUhBVn9-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windows/anaconda3/envs/vision/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2 , os, numpy as np, torch, pandas as pd, tqdm as tqdm, PIL.Image as Image, time, IPython\n",
    "#from pylab import rcParams\n",
    "import datetime\n",
    "# \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "#import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loops.fit_loop import FitLoop\n",
    "from pytorch_lightning.loops.loop import Loop\n",
    "from albumentations.pytorch.transforms import ToTensorV2 \n",
    "#\n",
    "from numpy.lib.function_base import select\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import pytorch_lightning as pl, torchmetrics\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F3Rn6pJjXGPh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'proj_path': PosixPath('/mnt/c/data/MattM_Ensemble'), 'model_path': PosixPath('/mnt/c/data/MattM_Ensemble/model_chkpts'), 'data_path': PosixPath('/mnt/c/data'), 'data_name': ['HE_RBG_Corp_images'], 'dataindex_fn': 'HE_RBG_Corp_images_processed/dataIndex(ubuntu).csv', 'dataindex_path': PosixPath('/mnt/c/data/HE_RBG_Corp_images_processed/dataIndex(ubuntu).csv'), 'class_names': ['Responder', 'NonResponder'], '__dict__': <attribute '__dict__' of 'PATH_ARGS' objects>, '__weakref__': <attribute '__weakref__' of 'PATH_ARGS' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "class PATH_ARGS:\n",
    "    proj_path = Path('./').absolute()  # [CHANGE THIS for new environment]\n",
    "    model_path = proj_path/'model_chkpts'\n",
    "    # data path\n",
    "    #data_path = proj_path/'TestingData'   # Test path\n",
    "    #data_path = proj_path/'Ensemble_learning data'      # [CONFIRM THIS for new environment]\n",
    "    data_path = proj_path.parent\n",
    "    # 2 types of images (HE  FISH)\n",
    "    data_name = ['HE_RBG_Corp_images']\n",
    "    dataindex_fn = data_name[0]+'_processed/dataIndex(ubuntu).csv'\n",
    "    dataindex_path = data_path/dataindex_fn\n",
    "    #data_name = ['HE images', 'HIPT_AGH_FluorescentImage_R1']\n",
    "    # 2 groups to classify\n",
    "    class_names = ['Responder','NonResponder']\n",
    "    \n",
    "print(PATH_ARGS.__dict__)\n",
    "def mkdirifNE(p):\n",
    "    if not os.path.exists(p): os.mkdir(p)\n",
    "\n",
    "mkdirifNE(PATH_ARGS.model_path)\n",
    "\n",
    "def load_img(img_paths: list, is_mask=False):\n",
    "        \"\"\" load array from a list of image paths \"\"\"\n",
    "        if is_mask: flag = 0\n",
    "        else: flag = -1\n",
    "        return np.concatenate([np.expand_dims(cv2.imread(str(img_fp), flag), axis=0)\n",
    "                               for img_fp in img_paths.tolist()])\n",
    "def normalize(ratios):\n",
    "    \"\"\"normalize a list of ratios to sum to 1\"\"\"\n",
    "    return [r/sum(ratios) for r in ratios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IC1aUI5cbwld"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent_path</th>\n",
       "      <th>type</th>\n",
       "      <th>tissue</th>\n",
       "      <th>x_img_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">/mnt/c/data/HE_RBG_Corp_images_processed/NonResponder</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">HE_RBG_Corp_images</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638).tif</th>\n",
       "      <th>NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638)_t0.tif</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA-24_0000023786_2021-03-22 09_49_45.scn - Series 1 (1, x=16274, y=23991, w=3780, h=3638)_t1.tif</th>\n",
       "      <td>NonResponder</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                    label  \\\n",
       "parent_path                                        type               tissue                                             x_img_path                                                         \n",
       "/mnt/c/data/HE_RBG_Corp_images_processed/NonRes... HE_RBG_Corp_images NA-24_0000023786_2021-03-22 09_49_45.scn - Seri... NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  NonResponder   \n",
       "                                                                                                                         NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  NonResponder   \n",
       "\n",
       "                                                                                                                                                                               set  \n",
       "parent_path                                        type               tissue                                             x_img_path                                                 \n",
       "/mnt/c/data/HE_RBG_Corp_images_processed/NonRes... HE_RBG_Corp_images NA-24_0000023786_2021-03-22 09_49_45.scn - Seri... NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  train  \n",
       "                                                                                                                         NA-24_0000023786_2021-03-22 09_49_45.scn - Seri...  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index_df = pd.read_csv(PATH_ARGS.dataindex_path, index_col=list(range(4)))\n",
    "# print(data_index_df['set'].unique())\n",
    "# print(data_index_df['set'].value_counts())\n",
    "data_index_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjnyh2KKfp3L"
   },
   "source": [
    "### Dataloader - loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oVqkEUQlftyf"
   },
   "outputs": [],
   "source": [
    "class META_ARGS:\n",
    "    RANDOM_SEED = 42\n",
    "    INPUT_DIM = (224,224)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DATA_ARGS:\n",
    "    num_classes = 2\n",
    "    batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Rvj6sDvItzfJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 112, 112, 3])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_normalize_attributes(data_index_df):\n",
    "    x_imgs = load_img(data_index_df['x_img_path'])\n",
    "    means, stds = np.mean(x_imgs, axis=((0,1,2))), np.std(x_imgs, axis=((0,1,2)))\n",
    "    return means, stds\n",
    "\n",
    "def tile_images_basic(im:np.array, patch_dims=(224,224)):\n",
    "        \"\"\" return generator object\"\"\"\n",
    "        M, N = patch_dims\n",
    "        for y in range(0, im.shape[1]-N+1, N):\n",
    "            for x in range(0,im.shape[0]-M+1, M):\n",
    "                yield im[x:x+M, y:y+N,:]\n",
    "                \n",
    "# dataloader\n",
    "class HEData(Dataset):\n",
    "    def __init__(self, dataindex_df: pd.DataFrame,\n",
    "                 x_img_cols:str=['x_img_path'], y_cols:list=['label'],\n",
    "                 transform=None, target_transform=None, patch_size=None, debug:bool=False):\n",
    "        \"\"\" \n",
    "        parameters\n",
    "            csv_file: contain indexer file\n",
    "            \n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        # \n",
    "        self.n = len(dataindex_df)\n",
    "        # fetch individual \n",
    "        self.y_ds = dataindex_df[y_cols]\n",
    "        self.num_classes = self.y_ds.nunique()\n",
    "        self.y_ds_enc = self.label_encode(self.y_ds, oh=False)\n",
    "        # \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.debug:\n",
    "            print(f\"Target shape:{self.y_ds_enc.shape}\")\n",
    "            print(f\"[INFO]Image classes: {self.num_classes} with {self.n} instances.\")\n",
    "        self.patch_size = patch_size\n",
    "        # if not patch_size is None:\n",
    "        #     assert patch_size[0]>=224 and patch_size[0]%224==0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def label_encode(self, ys, oh:bool=False):\n",
    "        # encode target label\n",
    "        if oh:\n",
    "            self.enc = OneHotEncoder()\n",
    "            return self.enc.fit_transform(ys).toarray()\n",
    "        else:\n",
    "            self.enc = LabelBinarizer()\n",
    "        ys_enc = self.enc.fit_transform(ys)\n",
    "        return ys_enc.flatten()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input images\n",
    "        if self.debug: print(f\"Instance series: {self.y_ds.iloc[idx]},{self.y_ds.iloc[idx].name}, {idx}\")\n",
    "        parent_path, _, _, tile_name = self.y_ds.iloc[idx].name   # parent, type, source_tissue, tile_name\n",
    "            # get data\n",
    "        if not self.patch_size is None:\n",
    "            x_data = np.stack([x for x in tile_images_basic(np.array(Image.open(Path(parent_path)/tile_name)),\n",
    "                                                                  patch_dims=self.patch_size)])\n",
    "        else:\n",
    "            x_data = np.array(Image.open(Path(parent_path)/tile_name))\n",
    "        y_data = self.y_ds_enc[idx]    #.reshape((-1,))\n",
    "        if self.transform is not None:\n",
    "            x_data = [self.transform(image=x)['image'] for x in x_data]\n",
    "        if self.target_transform:\n",
    "            y_data = self.target_transform(y_data)\n",
    "        # outputs g(t)\n",
    "        if self.debug: print(x_data.shape, x_data.dtype, y_data.shape, y_data.dtype)\n",
    "        return (torch.tensor(x_data).float(), torch.tensor(y_data, dtype=torch.long).tile(len(x_data)))\n",
    "\n",
    "def get_transforms(target_size=(224,224), get_normalizing_attributes:bool=False, data_index_df:pd.DataFrame=False):    \n",
    "    assert bool(get_normalizing_attributes) == bool(data_index_df), \"must be provided together\"\n",
    "    p1 = 0.1\n",
    "    p2 = 0.05\n",
    "    p3 = 0.2\n",
    "\n",
    "    if get_normalizing_attributes:\n",
    "        im_means, im_stds = _get_normalize_attributes()\n",
    "    else:   # use pre-computed values\n",
    "        im_means, im_stds=[0, 0, 0], [1, 1, 1]\n",
    "    ## Transforms\n",
    "    process_transform = A.Compose([\n",
    "        ToTensorV2(),\n",
    "    ]) # Normalize by channel means, stds\n",
    "    color_transform = A.Compose([\n",
    "        # In-place transformations\n",
    "        A.RandomBrightnessContrast(p=p2),\n",
    "        A.RandomGamma(gamma_limit=(80, 200), p=p3),\n",
    "        A.Blur(blur_limit=7, p=p2),\n",
    "        A.ToGray(p=p2),\n",
    "        A.CLAHE(p=p2),\n",
    "        A.ChannelDropout(channel_drop_range=(1, 2), fill_value=0, p=p2),\n",
    "        A.ChannelShuffle(p=p2),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.2,\n",
    "            always_apply=False,\n",
    "            p=p2,\n",
    "        ),\n",
    "        A.Equalize(mode=\"cv\", by_channels=True, mask=None, mask_params=(), p=p2),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, p=p2),\n",
    "        A.Posterize(num_bits=4, p=p2),\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=p2),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=p1)\n",
    "        #A.GaussianBlur(11, sigma=(0.1, 2.0)),\n",
    "    ])\n",
    "    geometric_transform = A.Compose([\n",
    "        A.Affine(\n",
    "            scale=(0.60, 1.60),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            translate_percent=(-0.2, 0.2),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            rotate=(-30, 30),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            shear=(-20, 20),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval=0,\n",
    "            cval_mask=0,\n",
    "            mode=cv2.BORDER_CONSTANT,\n",
    "            fit_output=False,\n",
    "            p=p1,\n",
    "        ),\n",
    "        # A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=pt),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ])\n",
    "    ###\n",
    "    transformers = {'process': process_transform,  \n",
    "                    'color': color_transform, 'geometric': geometric_transform}\n",
    "    set_transformers = {'train': A.Compose(color_transform.transforms + geometric_transform.transforms+process_transform.transforms),\n",
    "                        'val': A.Compose(process_transform.transforms),\n",
    "                        'test': A.Compose(process_transform.transforms)}\n",
    "    return set_transformers\n",
    "    #return transformers\n",
    "    \n",
    "HE_data = HEData(dataindex_df=pd.read_csv(PATH_ARGS.dataindex_path,index_col=list(range(4))), patch_size=(112,112))\n",
    "HE_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "_iadyH-ar2kB"
   },
   "outputs": [],
   "source": [
    "# full dataset objecct\n",
    "class HEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, dataindex_path=Path('./dataIndex.csv'), label_col='label', \n",
    "                 patch_size=224, debug=False):\n",
    "        super().__init__()\n",
    "        self.dataindex_path = Path(dataindex_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = DATA_ARGS.num_classes\n",
    "        self.label_col = label_col\n",
    "        self.transforms = get_transforms()\n",
    "        self.debug = debug\n",
    "        print(f\"Debug mode:{self.debug}\")\n",
    "        self.index_col_len = 4\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def get_sampler(self, dataset):\n",
    "        \"\"\"get sampler if needed\"\"\"\n",
    "        if self.label_col:\n",
    "            class_cts = dataset[self.label_col].value_counts()\n",
    "            for label in class_cts.index:\n",
    "                class_cts.loc[label] = len(dataset)/class_cts.loc[label]\n",
    "            weights = np.zeros(len(dataset))\n",
    "            for label in class_cts.index:\n",
    "                weights[np.where(dataset[self.label_col].to_numpy()==label)[0]] = class_cts.loc[label]\n",
    "            class_balance_sampler = WeightedRandomSampler(weights, len(dataset), replacement=True)\n",
    "        else:\n",
    "            class_balance_sampler = None\n",
    "        return class_balance_sampler\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.datasets = dict()\n",
    "        self.sampler = dict()\n",
    "        # ['train', 'test', 'val']\n",
    "        dataindex_df = pd.read_csv(self.dataindex_path, index_col=list(range(self.index_col_len)))\n",
    "        dataindex_df = dataindex_df[dataindex_df['set'].isnull()!=True]\n",
    "        for dset in dataindex_df['set'].unique():\n",
    "            self.sampler[dset] = self.get_sampler(dataindex_df[dataindex_df['set']==dset])\n",
    "            self.datasets[dset] = HEData(dataindex_df[dataindex_df['set']==dset], patch_size=self.patch_size,\n",
    "                                         transform = self.transforms[dset], debug=self.debug)\n",
    "    \n",
    "    def custom_collate(self, batch):\n",
    "        return torch.cat([x for x, _ in batch]), torch.cat([y for _, y in batch])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.datasets['train'], batch_size=self.batch_size, shuffle=False if self.sampler['train'] else True, sampler=self.sampler['train'],\n",
    "            num_workers=64, pin_memory=True, collate_fn=self.custom_collate\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(\n",
    "            self.datasets['val'], batch_size=self.batch_size, shuffle=False if self.sampler['val'] else True, sampler=self.sampler['val'],\n",
    "            num_workers=64, pin_memory=True, collate_fn=self.custom_collate\n",
    "        )\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    return torch.cat([x for x, _ in batch]), torch.cat([y for _, y in batch])\n",
    "for x, y in DataLoader(HE_data, shuffle=False, collate_fn=custom_collate):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode:False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HEDataModule at 0x7f367d912fd0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HEDataModule(dataindex_path=Path('./dataIndex.csv'), patch_size=112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-2zwr80fr_0"
   },
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BZjWd492DsUb"
   },
   "outputs": [],
   "source": [
    "# model and train args\n",
    "class MODEL_ARGS:\n",
    "    n_classes = len(PATH_ARGS.class_names)\n",
    "    \n",
    "class TRAIN_ARGS:\n",
    "    batch_size = DATA_ARGS.batch_size\n",
    "    epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet_B7_Weights, ResNeXt101_32X8D_Weights, MobileNet_V3_Large_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gcE8La0ftMU4"
   },
   "outputs": [],
   "source": [
    "# returns the size of the output tensor going into Linear layer from the conv block.\n",
    "def _get_conv_output(self, shape):\n",
    "    batch_size = 1\n",
    "    input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "    output_feat = self._forward_features(input) \n",
    "    n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "    return n_size\n",
    "    \n",
    "#from torch._C import device\n",
    "class HEClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, model_name:str, n_classes:int=2, pretrain:bool=True,\n",
    "                 custom_classification_head:bool=False, input_size:tuple=(224,224), debug:bool=False):\n",
    "        super().__init__()\n",
    "        print(f\"Using pre-trained head:{model_name}\")\n",
    "        avail_models =  ['mobilenetv3','resnext101','efficientnetb7','resnet50']\n",
    "        assert model_name in ['mobilenetv3','resnext101','efficientnetb7','resnet50'], f\"Must be one of {avail_models}\"\n",
    "        self.debug = debug\n",
    "        self.n_classes = n_classes\n",
    "        self.custom_classification_head = custom_classification_head\n",
    "        # Step 1: Initialize model with the weights\n",
    "        if model_name == 'mobilenetv3':\n",
    "            self.model = models.mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrain else None)\n",
    "        elif model_name == 'resnext101':\n",
    "            self.model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1 if pretrain else None)\n",
    "        elif model_name == 'efficientnetb7':\n",
    "            self.model = models.efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1 if pretrain else None)\n",
    "        elif model_name =='resnet50':\n",
    "            self.model = models.resnet50(pretrained=ResNet50_Weights.IMAGENET1K_V2 if pretrain else None)\n",
    "        # replace/remove head\n",
    "        removed = list(self.model.children())[:-1]\n",
    "        self.model_base = torch.nn.Sequential(*removed)  \n",
    "        in_feats = self._get_output_feat(self.model_base, input_size)\n",
    "            # head\n",
    "        if self.custom_classification_head:\n",
    "            self.model_head = self.classification_head()\n",
    "        else:\n",
    "            self.model_head = nn.Sequential(nn.Flatten(),\n",
    "                                            nn.Linear(in_features=in_feats, out_features=self.n_classes, bias=True),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.LogSoftmax(dim=1) if n_classes>2 else nn.Sigmoid(),\n",
    "                                           )\n",
    "        self.model = torch.nn.Sequential(self.model_base, self.model_head)\n",
    "            #self.model_head.to(device=META_ARGS.device)     \n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        #self.ROC = torchmetrics.ROC(num_classes=n_classes)\n",
    "        self.AUROC = torchmetrics.AUROC(num_classes=n_classes, pos_label=1)\n",
    "    \n",
    "    def _get_output_feat(self, model, in_shape=(224,224)):\n",
    "        x = torch.randn((3,)+in_shape)\n",
    "        return model(x.unsqueeze(0)).flatten().size()[0]\n",
    "\n",
    "    def _forward_feature_extract(self, x):\n",
    "        return self.model_base(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "#         #x = self.model_head(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(nn.Linear(in_features=self.model.classifier[-1].in_features, out_features=self.n_classes, bias=True)(x))\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        #self.model.classifier = nn.Sequential(*self.model.classifier, nn.Softmax())\n",
    "        if self.debug: print(f\"Num classes:{self.n_classes}\\nModel classifier\\n:{self.model_head}\")\n",
    "        return x\n",
    "\n",
    "    def add_classification_head(self):\n",
    "        #n_features = self.model_head.fc.in_features\n",
    "        classifier_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.model_base.classifier[1].in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512 , 256),\n",
    "            nn.Linear(256 , self.n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "            )\n",
    "        return classifier_layer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-10)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def get_loss(self, y_hat, y):\n",
    "        #loss = nn.CrossEntropyLoss()   # does softmax for you (no need in classifcation)\n",
    "        #loss = nn.LogSoftmax()\n",
    "        #loss = F.nll_loss\n",
    "        if self.debug: print(y.size(), y.dtype, y_hat.size(), y_hat.dtype)\n",
    "        return F.cross_entropy(y_hat,  y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.get_loss(y_hat, y)\n",
    "        # training metrics\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        # optimize (done under the hoood)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "        #return self.get_loss(y, y_hat)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # compute metrics\n",
    "        val_loss =self.get_loss(y_hat, y)\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        auroc = self.AUROC(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        #fpr, tpr, thresholds = self.ROC(y_hat, y)\n",
    "        #\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        self.AUROC.update(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        self.log(\"validation_auc\", self.AUROC, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_auc\", valid_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "\n",
    "class HEEnsembleModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 ensembles_settings:dict={'efficientnetb7':3, 'resnext101':2}, \n",
    "                 pretrain:bool=True,\n",
    "                 n_classes:int=2,\n",
    "                 input_shape=(224,224),\n",
    "                 debug=False):\n",
    "        super(HEEnsembleModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        models = []\n",
    "        self.n_models = 0\n",
    "        for name, number in ensembles_settings.items():\n",
    "            [models.append(\n",
    "                HEClassificationModel(model_name=name, \n",
    "                                      n_classes=2, \n",
    "                                      pretrain=pretrain,\n",
    "                                      custom_classification_head=False\n",
    "                                     )\n",
    "                         ) for i in range(number)\n",
    "            ]\n",
    "            self.n_models += number\n",
    "        self.ensemble_model = torch.nn.ModuleList(models)\n",
    "        self.classifier = torch.nn.Linear(self.n_models*n_classes, n_classes)\n",
    "        #self.save_hyperparameters() # Uncomment to show error\n",
    "        self.CEloss = nn.CrossEntropyLoss()\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        #self.ROC = torchmetrics.ROC(num_classes=n_classes)\n",
    "        self.AUROC = torchmetrics.AUROC(num_classes=n_classes, pos_label=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output=[]\n",
    "        for m in self.ensemble_model:\n",
    "            output.append(m(x))\n",
    "        combined = torch.concat(output,dim=1)\n",
    "        x = self.classifier(combined)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def get_loss(self, y_hat, y):\n",
    "        #loss = nn.CrossEntropyLoss()   # does softmax for you (no need in classifcation)\n",
    "        #loss = nn.LogSoftmax()\n",
    "        #loss = F.nll_loss\n",
    "        if self.debug: print(y.size(), y.dtype, y_hat.size(), y_hat.dtype)\n",
    "        return self.CEloss(y_hat,  y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.get_loss(y_hat, y)\n",
    "        # training metrics\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        # optimize (done under the hoood)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "        #return self.get_loss(y, y_hat)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # compute metrics\n",
    "        val_loss =self.get_loss(y_hat, y)\n",
    "        acc = self.accuracy(torch.argmax(y_hat, dim=1), y)\n",
    "        auroc = self.AUROC(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        #fpr, tpr, thresholds = self.ROC(y_hat, y)\n",
    "        #\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        self.AUROC.update(y_hat.cpu().detach(), y.cpu().detach())\n",
    "        self.log(\"validation_auc\", self.AUROC, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_auc\", valid_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl1KS1Qdq_05"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Kinjz3AgaGjB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from pytorch_lightning.callbacks import EarlyStopping, GradientAccumulationScheduler\n",
    "import pytorch_lightning as pl\n",
    "# logger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "#\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Gk2iGQLALW7W"
   },
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "        Try resetting model weights to avoid\n",
    "        weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1aoYeC2oNT2A",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn|ddp_fork). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DEFAULT (ie: no accumulated grads)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, dirpath\u001b[38;5;241m=\u001b[39mPATH_ARGS\u001b[38;5;241m.\u001b[39mmodel_path,\n\u001b[0;32m      4\u001b[0m                                  filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels-\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_loss:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#PRMetrics(),\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m----> 9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWandbLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAD-ensemble(draft)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_ARGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddp_spawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#trainer = Trainer(accelerator=\"gpu\", devices=2, num_nodes=4)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m HEEnsembleModel(ensembles_settings\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefficientnetb7\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m                                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobilenetv3\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     19\u001b[0m                                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnext101\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m                         n_classes\u001b[38;5;241m=\u001b[39mMODEL_ARGS\u001b[38;5;241m.\u001b[39mn_classes,\n\u001b[0;32m     23\u001b[0m                         debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:345\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:433\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:230\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_init_precision()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensemble_vision\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:817\u001b[0m, in \u001b[0;36mAcceleratorConnector._lazy_init_strategy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_INTERACTIVE\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mis_interactive_compatible:\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer(strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mstrategy_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` is not compatible with an interactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m environment. Run your code as a script, or choose one of the compatible strategies:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Trainer(strategy=None|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_StrategyType\u001b[38;5;241m.\u001b[39minteractive_compatible_types())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m creation inside the worker function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m     )\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;66;03m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, TPUAccelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, (SingleTPUStrategy, TPUSpawnStrategy)\n\u001b[0;32m    829\u001b[0m ):\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn|ddp_fork). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: While tearing down the service manager. The following error has occured: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT (ie: no accumulated grads)\n",
    "cbs = [\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss', dirpath=PATH_ARGS.model_path,\n",
    "                                 filename='models-{epoch:02d}-{val_loss:.2f}', save_top_k=2, mode='min'),\n",
    "    EarlyStopping(monitor=\"val_loss\", min_delta=1e-7, patience=8, mode=\"min\"),\n",
    "    GradientAccumulationScheduler(scheduling={0: 8, 4: 4, 8: 1}),\n",
    "    #PRMetrics(),\n",
    "]\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=2,\n",
    "    logger=WandbLogger(project='AD-ensemble(draft)',  entity=\"3m-m\", job_type='train'),\n",
    "    max_epochs=TRAIN_ARGS.epochs, callbacks=cbs,\n",
    "    strategy='ddp_spawn'\n",
    ")\n",
    "#trainer = Trainer(accelerator=\"gpu\", devices=2, num_nodes=4)\n",
    "model = HEEnsembleModel(ensembles_settings={'efficientnetb7':1,\n",
    "                                            'mobilenetv3':2,\n",
    "                                            'resnext101':2},\n",
    "                        pretrain=False,\n",
    "                        input_shape=(224,224),\n",
    "                        n_classes=MODEL_ARGS.n_classes,\n",
    "                        debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBQBmVGE3u_M"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "datamodule = HEDataModule(batch_size=TRAIN_ARGS.epochs, dataindex_path=PATH_ARGS.dataindex_path, debug=False)\n",
    "datamodule.setup()\n",
    "trainer.fit(model=model, datamodule=datamodule) \n",
    "print(\"Done\")\n",
    "# save with parameters\n",
    "#torch.save([model.kwargs, model.state_dict()], path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v65ctfW7HABS"
   },
   "source": [
    "### Prediction/submission"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "uUsLsFHLv8b8"
   },
   "source": [
    "test_loader = DataLoader(HEdatasets['test'], shuffle=True, batch_size=TRAIN_args.test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dca68bea1ee5b3be888f6b0e1f9475265928937e00d73db3ed64c34751d2276f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
